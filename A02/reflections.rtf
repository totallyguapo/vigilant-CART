{\rtf1\ansi\ansicpg1252\cocoartf2709
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\fnil\fcharset0 Menlo-Regular;}
{\colortbl;\red255\green255\blue255;\red24\green24\blue24;\red255\green255\blue255;}
{\*\expandedcolortbl;;\cssrgb\c12157\c12157\c12157;\cssrgb\c100000\c100000\c100000;}
\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 As the number of degrees past the highest probability guess increased: the text became increasingly nonsensical. Somewhere before losing coherence: around degree 40 in my experiment, the text reached the ideal mix of absurdism while maintaining coherence. For instance in: \
\
Of the pine-trees crusted with  seeds\
And have been cold a long  Time\
\'85\
he spruces rough in the distant  years\
\'85\
Of any misery in the sound of the  old\
\
A theme of age adds to the snowy atmosphere of the poem, and repeats appropriately throughout the poem. \
\
In contrast to degree 69 which loses coherence: \
\pard\pardeftab720\partightenfactor0

\f1\fs28 \cf2 \cb3 \expnd0\expndtw0\kerning0
One must have a mind of  at\
To regard the frost and the  p\
Of the pine-trees crusted with  golden\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 \cb1 \kerning1\expnd0\expndtw0 Although the last line is interesting, the first two don\'92t mean much of anything. The model is reduced to low guessing fragment tokens whose purpose is to connect to a new idea, rather than predicting a token representing a meaningful word, or idea. \
\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\'97\
To train a model to find the nouns within a poem is challenging because the model does not have a method to classify symbolic abstractions like nouns\'97and words can act as various grammatical classifications. But because the model is already doing the work of establishing a context through its tokenizations, an additional layer could be added after the tokenizations, which maps each group of tokens associated with a word, to one type of word: nouns, adjectives, verbs, adverbs. Each type could be associated to a number.\
\
The model could then reprocess the input by assigning probabilities of each token-group\'92s association to each word type. \
The output would not perfectly classify the grammar, but with a large enough dataset, could get extremely close.}