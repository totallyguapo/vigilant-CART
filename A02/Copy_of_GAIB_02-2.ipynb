{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Activity on predicting probabilities for a next token"
      ],
      "metadata": {
        "id": "XAGTMLK9_giH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Using the GPT-2 language model, you will replace the last word of each line from The Snow Man with the word that has the seventh-highest probability according to the model’s predictions.\n",
        "# here it is:\n",
        "# \"The Snow Man\n",
        "# by Wallace Stevens (1879-1955)\n",
        "# One must have a mind of winter\n",
        "# To regard the frost and the boughs\n",
        "# Of the pine-trees crusted with snow;\n",
        "# And have been cold a long time\n",
        "# To behold the junipers shagged with ice,\n",
        "# The spruces rough in the distant glitter\n",
        "# Of the January sun; and not to think\n",
        "# Of any misery in the sound of the wind,\n",
        "# In the sound of a few leaves,\n",
        "# Which is the sound of the land\n",
        "# Full of the same wind\n",
        "# That is blowing in the same bare place\n",
        "# For the listener, who listens in the snow,\n",
        "# And, nothing himself, beholds\n",
        "# Nothing that is not there and the nothing that is.\n",
        "# \"\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "def predict_next_tokens(prompt, model_name, num_tokens=10):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_ids).logits\n",
        "\n",
        "    next_token_logits = logits[0, -1, :]\n",
        "    probabilities = torch.softmax(next_token_logits, dim=-1)\n",
        "\n",
        "    top_probabilities, top_indices = torch.topk(probabilities, num_tokens)\n",
        "\n",
        "    predicted_tokens = []\n",
        "    for i in range(num_tokens):\n",
        "        token_id = top_indices[i].item()\n",
        "        token_text = tokenizer.decode(token_id)\n",
        "        predicted_tokens.append((token_text, top_probabilities[i].item()))\n",
        "\n",
        "    return predicted_tokens\n",
        "\n",
        "poem = \"\"\"\n",
        "One must have a mind of winter\n",
        "To regard the frost and the boughs\n",
        "Of the pine-trees crusted with snow;\n",
        "And have been cold a long time\n",
        "To behold the junipers shagged with ice,\n",
        "The spruces rough in the distant glitter\n",
        "Of the January sun; and not to think\n",
        "Of any misery in the sound of the wind,\n",
        "In the sound of a few leaves,\n",
        "Which is the sound of the land\n",
        "Full of the same wind\n",
        "That is blowing in the same bare place\n",
        "For the listener, who listens in the snow,\n",
        "And, nothing himself, beholds\n",
        "Nothing that is not there and the nothing that is.\n",
        "\"\"\"\n",
        "\n",
        "model_name = \"gpt2\" # You can change this to another GPT-2 variant if you want\n",
        "\n",
        "for line in poem.strip().split('\\n'):\n",
        "  if not line.strip():\n",
        "    continue\n",
        "  words = line.split()\n",
        "  if not words:\n",
        "    continue\n",
        "\n",
        "  prompt = \" \".join(words[:-1])\n",
        "  predictions = predict_next_tokens(prompt, model_name, num_tokens=10)\n",
        "\n",
        "  # Replace with the 7th most probable word (index 6)\n",
        "  if len(predictions) >= 7:\n",
        "    new_word = predictions[6][0]\n",
        "    new_line = prompt + \" \" + new_word\n",
        "    print(new_line)\n",
        "  else:\n",
        "    print(line) # Print the original line if there are not enough predictions"
      ],
      "metadata": {
        "id": "Ar655eQg6UaU",
        "outputId": "b422c87e-0d81-4fbe-dd17-849ca5a8ca19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One must have a mind of  her\n",
            "To regard the frost and the  death\n",
            "Of the pine-trees crusted with  oil\n",
            "And have been cold a long  way\n",
            "To behold the junipers shagged with  white\n",
            "The spruces rough in the distant  horizon\n",
            "Of the January sun; and not to  have\n",
            "Of any misery in the sound of the  sound\n",
            "In the sound of a few  shots\n",
            "Which is the sound of the  voice\n",
            "Full of the same  day\n",
            "That is blowing in the same bare  air\n",
            "For the listener, who listens in the  morning\n",
            "And, nothing himself,  I\n",
            "Nothing that is not there and the nothing that  isn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Using the GPT-2 language model, you will replace the last word of each line from The Snow Man with the word that has the seventh-highest probability according to the model’s predictions.\n",
        "# here it is:\n",
        "# \"The Snow Man\n",
        "# by Wallace Stevens (1879-1955)\n",
        "# One must have a mind of winter\n",
        "# To regard the frost and the boughs\n",
        "# Of the pine-trees crusted with snow;\n",
        "# And have been cold a long time\n",
        "# To behold the junipers shagged with ice,\n",
        "# The spruces rough in the distant glitter\n",
        "# Of the January sun; and not to think\n",
        "# Of any misery in the sound of the wind,\n",
        "# In the sound of a few leaves,\n",
        "# Which is the sound of the land\n",
        "# Full of the same wind\n",
        "# That is blowing in the same bare place\n",
        "# For the listener, who listens in the snow,\n",
        "# And, nothing himself, beholds\n",
        "# Nothing that is not there and the nothing that is.\n",
        "# \"\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "def predict_next_tokens(prompt, model_name, num_tokens=10):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_ids).logits\n",
        "\n",
        "    next_token_logits = logits[0, -1, :]\n",
        "    probabilities = torch.softmax(next_token_logits, dim=-1)\n",
        "\n",
        "    top_probabilities, top_indices = torch.topk(probabilities, num_tokens)\n",
        "\n",
        "    predicted_tokens = []\n",
        "    for i in range(num_tokens):\n",
        "        token_id = top_indices[i].item()\n",
        "        token_text = tokenizer.decode(token_id)\n",
        "        predicted_tokens.append((token_text, top_probabilities[i].item()))\n",
        "\n",
        "    return predicted_tokens\n",
        "\n",
        "poem = \"\"\"\n",
        "One must have a mind of winter\n",
        "To regard the frost and the boughs\n",
        "Of the pine-trees crusted with snow;\n",
        "And have been cold a long time\n",
        "To behold the junipers shagged with ice,\n",
        "The spruces rough in the distant glitter\n",
        "Of the January sun; and not to think\n",
        "Of any misery in the sound of the wind,\n",
        "In the sound of a few leaves,\n",
        "Which is the sound of the land\n",
        "Full of the same wind\n",
        "That is blowing in the same bare place\n",
        "For the listener, who listens in the snow,\n",
        "And, nothing himself, beholds\n",
        "Nothing that is not there and the nothing that is.\n",
        "\"\"\"\n",
        "\n",
        "model_name = \"gpt2\" # You can change this to another GPT-2 variant if you want\n",
        "\n",
        "for line in poem.strip().split('\\n'):\n",
        "  if not line.strip():\n",
        "    continue\n",
        "  words = line.split()\n",
        "  if not words:\n",
        "    continue\n",
        "\n",
        "  prompt = \" \".join(words[:-1])\n",
        "  predictions = predict_next_tokens(prompt, model_name, num_tokens=10)\n",
        "\n",
        "  # Replace with the 7th most probable word (index 69)\n",
        "  if len(predictions) >= 10:\n",
        "    new_word = predictions[9][0]\n",
        "    new_line = prompt + \" \" + new_word\n",
        "    print(new_line)\n",
        "  else:\n",
        "    print(line) # Print the original line if there are not enough predictions"
      ],
      "metadata": {
        "id": "FonT0K3i7BjW",
        "outputId": "eff7101a-3543-4fac-cccb-e9689e2201ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One must have a mind of  one\n",
            "To regard the frost and the  storm\n",
            "Of the pine-trees crusted with  white\n",
            "And have been cold a long  and\n",
            "To behold the junipers shagged with  tw\n",
            "The spruces rough in the distant  distance\n",
            "Of the January sun; and not to  my\n",
            "Of any misery in the sound of the  earth\n",
            "In the sound of a few  loud\n",
            "Which is the sound of the  music\n",
            "Full of the same  time\n",
            "That is blowing in the same bare  bones\n",
            "For the listener, who listens in the  studio\n",
            "And, nothing himself,  though\n",
            "Nothing that is not there and the nothing that  comes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "def predict_next_tokens(prompt, model_name, num_tokens=10):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_ids).logits\n",
        "\n",
        "    next_token_logits = logits[0, -1, :]\n",
        "    probabilities = torch.softmax(next_token_logits, dim=-1)\n",
        "\n",
        "    top_probabilities, top_indices = torch.topk(probabilities, num_tokens)\n",
        "\n",
        "    predicted_tokens = []\n",
        "    for i in range(num_tokens):\n",
        "        token_id = top_indices[i].item()\n",
        "        token_text = tokenizer.decode(token_id)\n",
        "        predicted_tokens.append((token_text, top_probabilities[i].item()))\n",
        "\n",
        "    return predicted_tokens\n",
        "\n",
        "poem = \"\"\"\n",
        "One must have a mind of winter\n",
        "To regard the frost and the boughs\n",
        "Of the pine-trees crusted with snow;\n",
        "And have been cold a long time\n",
        "To behold the junipers shagged with ice,\n",
        "The spruces rough in the distant glitter\n",
        "Of the January sun; and not to think\n",
        "Of any misery in the sound of the wind,\n",
        "In the sound of a few leaves,\n",
        "Which is the sound of the land\n",
        "Full of the same wind\n",
        "That is blowing in the same bare place\n",
        "For the listener, who listens in the snow,\n",
        "And, nothing himself, beholds\n",
        "Nothing that is not there and the nothing that is.\n",
        "\"\"\"\n",
        "\n",
        "model_name = \"gpt2\" # You can change this to another GPT-2 variant if you want\n",
        "\n",
        "for line in poem.strip().split('\\n'):\n",
        "  if not line.strip():\n",
        "    continue\n",
        "  words = line.split()\n",
        "  if not words:\n",
        "    continue\n",
        "\n",
        "  prompt = \" \".join(words[:-1])\n",
        "  predictions = predict_next_tokens(prompt, model_name, num_tokens=40)\n",
        "\n",
        "  # Replace with the 40th most probable word (index 40)\n",
        "  if len(predictions) >= 40:\n",
        "    new_word = predictions[39][0]\n",
        "    new_line = prompt + \" \" + new_word\n",
        "    print(new_line)\n",
        "  else:\n",
        "    print(line) # Print the original line if there are not enough predictions"
      ],
      "metadata": {
        "id": "Xh3wmt1X_xcH",
        "outputId": "73e43f0b-45f8-407b-aee0-d9bc441eb21c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One must have a mind of  consistency\n",
            "To regard the frost and the  effect\n",
            "Of the pine-trees crusted with  seeds\n",
            "And have been cold a long  Time\n",
            "To behold the junipers shagged with  thick\n",
            "The spruces rough in the distant  years\n",
            "Of the January sun; and not to  him\n",
            "Of any misery in the sound of the  old\n",
            "In the sound of a few  notes\n",
            "Which is the sound of the  ball\n",
            "Full of the same  order\n",
            "That is blowing in the same bare  hole\n",
            "For the listener, who listens in the  normal\n",
            "And, nothing himself,  other\n",
            "Nothing that is not there and the nothing that  must\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Using the GPT-2 language model, you will replace the last word of each line from The Snow Man with the word that has the seventh-highest probability according to the model’s predictions.\n",
        "# here it is:\n",
        "# \"The Snow Man\n",
        "# by Wallace Stevens (1879-1955)\n",
        "# One must have a mind of winter\n",
        "# To regard the frost and the boughs\n",
        "# Of the pine-trees crusted with snow;\n",
        "# And have been cold a long time\n",
        "# To behold the junipers shagged with ice,\n",
        "# The spruces rough in the distant glitter\n",
        "# Of the January sun; and not to think\n",
        "# Of any misery in the sound of the wind,\n",
        "# In the sound of a few leaves,\n",
        "# Which is the sound of the land\n",
        "# Full of the same wind\n",
        "# That is blowing in the same bare place\n",
        "# For the listener, who listens in the snow,\n",
        "# And, nothing himself, beholds\n",
        "# Nothing that is not there and the nothing that is.\n",
        "# \"\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "def predict_next_tokens(prompt, model_name, num_tokens=10):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_ids).logits\n",
        "\n",
        "    next_token_logits = logits[0, -1, :]\n",
        "    probabilities = torch.softmax(next_token_logits, dim=-1)\n",
        "\n",
        "    top_probabilities, top_indices = torch.topk(probabilities, num_tokens)\n",
        "\n",
        "    predicted_tokens = []\n",
        "    for i in range(num_tokens):\n",
        "        token_id = top_indices[i].item()\n",
        "        token_text = tokenizer.decode(token_id)\n",
        "        predicted_tokens.append((token_text, top_probabilities[i].item()))\n",
        "\n",
        "    return predicted_tokens\n",
        "\n",
        "poem = \"\"\"\n",
        "One must have a mind of winter\n",
        "To regard the frost and the boughs\n",
        "Of the pine-trees crusted with snow;\n",
        "And have been cold a long time\n",
        "To behold the junipers shagged with ice,\n",
        "The spruces rough in the distant glitter\n",
        "Of the January sun; and not to think\n",
        "Of any misery in the sound of the wind,\n",
        "In the sound of a few leaves,\n",
        "Which is the sound of the land\n",
        "Full of the same wind\n",
        "That is blowing in the same bare place\n",
        "For the listener, who listens in the snow,\n",
        "And, nothing himself, beholds\n",
        "Nothing that is not there and the nothing that is.\n",
        "\"\"\"\n",
        "\n",
        "model_name = \"gpt2\" # You can change this to another GPT-2 variant if you want\n",
        "\n",
        "for line in poem.strip().split('\\n'):\n",
        "  if not line.strip():\n",
        "    continue\n",
        "  words = line.split()\n",
        "  if not words:\n",
        "    continue\n",
        "\n",
        "  prompt = \" \".join(words[:-1])\n",
        "  predictions = predict_next_tokens(prompt, model_name, num_tokens=69)\n",
        "\n",
        "  # Replace with the 7th most probable word (index 69)\n",
        "  if len(predictions) >= 69:\n",
        "    new_word = predictions[68][0]\n",
        "    new_line = prompt + \" \" + new_word\n",
        "    print(new_line)\n",
        "  else:\n",
        "    print(line) # Print the original line if there are not enough predictions"
      ],
      "metadata": {
        "id": "5fEjEbgkAH_U",
        "outputId": "628875ed-870c-4088-a0b2-21cedb62c7cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One must have a mind of  at\n",
            "To regard the frost and the  p\n",
            "Of the pine-trees crusted with  golden\n",
            "And have been cold a long  for\n",
            "To behold the junipers shagged with  wings\n",
            "The spruces rough in the distant  wilderness\n",
            "Of the January sun; and not to  her\n",
            "Of any misery in the sound of the  morning\n",
            "In the sound of a few  bell\n",
            "Which is the sound of the  radio\n",
            "Full of the same  season\n",
            "That is blowing in the same bare  voice\n",
            "For the listener, who listens in the  home\n",
            "And, nothing himself,  anyway\n",
            "Nothing that is not there and the nothing that  flows\n"
          ]
        }
      ]
    }
  ]
}